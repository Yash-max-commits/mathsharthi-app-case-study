# Audio Pipeline — TTS/STT Architecture

## Overview

The AI Tutor module implements a **bidirectional voice interaction system**: users speak queries via microphone (STT path), and the AI responds with synthesized audio (TTS path). Both paths are **server-side** — the mobile client handles recording, playback, and UI synchronization, while transcription and speech synthesis happen on the backend.

---

## End-to-End Flow

```
                    ┌─────────────────────────────────┐
                    │        USER INTERACTION          │
                    └─────────┬───────────────────────┘
                              │
              ┌───────────────┼───────────────┐
              ▼                               ▼
     ┌────────────────┐              ┌────────────────┐
     │   STT Path     │              │   TTS Path     │
     │  (Voice Input) │              │ (Voice Output) │
     └───────┬────────┘              └───────┬────────┘
             │                               │
     ┌───────▼────────┐              ┌───────▼────────┐
     │ Record WAV     │              │ Receive base64 │
     │ (expo-av)      │              │ MP3 from API   │
     └───────┬────────┘              └───────┬────────┘
             │                               │
     ┌───────▼────────┐              ┌───────▼────────┐
     │ Upload to      │              │ Write to cache │
     │ backend        │              │ (FileSystem)   │
     └───────┬────────┘              └───────┬────────┘
             │                               │
     ┌───────▼────────┐              ┌───────▼────────┐
     │ Receive        │              │ Play audio     │
     │ transcript     │              │ (Audio.Sound)  │
     └───────┬────────┘              └───────┬────────┘
             │                               │
     ┌───────▼────────┐              ┌───────▼────────┐
     │ Populate query │              │ On finish:     │
     │ input field    │              │ delete cache   │
     └────────────────┘              └────────────────┘
```

---

## STT Path (Speech-to-Text)

### 1. Permission Request

```
Audio.requestPermissionsAsync()
→ If denied: Alert with message directing to device settings
→ If granted: proceed to recording
```

### 2. Audio Mode Configuration

Before recording, the audio session is configured for recording compatibility:

```
Audio.setAudioModeAsync({
  allowsRecordingIOS: true,       // Enable recording on iOS
  playsInSilentModeIOS: true      // Allow playback even in silent mode
})
```

### 3. Recording Configuration

Platform-specific codec settings ensure consistent WAV output:

| Setting | Android | iOS |
|---|---|---|
| Extension | `.wav` | `.wav` |
| Format | WAV | — |
| Encoder | PCM 16-bit | — |
| Audio Quality | — | Max |
| Sample Rate | 16,000 Hz | 16,000 Hz |
| Channels | 1 (mono) | 1 (mono) |
| Bit Rate | 256,000 bps | 256,000 bps |
| PCM Bit Depth | — | 16 |
| PCM Endianness | — | Little-endian |
| PCM Float | — | No |

**Why 16kHz mono WAV:** This is the standard input format for most speech recognition engines — sufficient quality for voice commands while minimizing file size.

### 4. Upload & Transcription

The recorded WAV is uploaded as multipart form data with an `action: 'transcribe'` flag. The backend returns a `{ transcript }` response, which is inserted into the text query input field for user review before sending.

### 5. Cleanup

The local WAV file is deleted immediately after upload (or on cancel):

```
FileSystem.deleteAsync(audioUri, { idempotent: true })
```

---

## TTS Path (Text-to-Speech)

### 1. Receive Audio from API

When the AI Tutor API responds with `audio_content`, it contains a **base64-encoded MP3** generated by Google Cloud Text-to-Speech on the backend.

### 2. Decode & Cache

The base64 string is written to a temporary file in the app's cache directory:

```
Path: ${FileSystem.cacheDirectory}response_${timestamp}.mp3
Encoding: Base64
```

### 3. Playback

An `Audio.Sound` instance is created and set to auto-play:

```
Audio.Sound.createAsync(
  { uri: cachedFilePath },
  { shouldPlay: true }
)
```

### 4. Status Monitoring & Cleanup

A playback status listener handles completion and cleanup:

```
sound.setOnPlaybackStatusUpdate((status) => {
  if (status.didJustFinish || status.isLoaded === false) {
    → setIsSpeaking(false)
    → setSound(null)
    → FileSystem.deleteAsync(cachedMp3)
  }
})
```

### 5. Interruption Handling

If the user navigates away mid-playback (back button), the sound is stopped and unloaded:

```
sound.stopAsync()  →  sound.unloadAsync()  →  setSound(null)
```

This is handled via `BackHandler` and a cleanup function in the `useEffect` return.

---

## Device Variability Issues

### Why On-Device Engines Were Abandoned

The project initially used `react-native-tts` (on-device TTS) and `react-native-voice` (on-device STT). These were abandoned due to:

| Issue | Detail |
|---|---|
| **Voice inconsistency** | Different Android manufacturers (Samsung, OnePlus, Xiaomi) ship different TTS engines with varying voice quality and language support |
| **Initialization failures** | Some devices failed to initialize the TTS engine silently — no error thrown, just no audio output |
| **Language availability** | Hindi voice support was inconsistent across devices; some required manual TTS data downloads |
| **STT accuracy** | On-device speech recognition varied significantly in accuracy across devices and Android versions |

**Evidence:** The `expo.doctor.exclude` configuration lists `react-native-tts` and `react-native-voice` as excluded packages — they remain in the dependency resolution graph but are no longer used in active code.

### Migration to Server-Side

Moving TTS/STT to the backend (Google Cloud) solved all device variability issues:

| Before (On-Device) | After (Server-Side) |
|---|---|
| Voice varies by device manufacturer | Consistent Google Cloud voice across all devices |
| Hindi support depends on installed TTS data | Server handles all language generation |
| STT accuracy varies by Android version | Server-side model provides consistent accuracy |
| No network required | Requires network (acceptable for an AI app) |

---

## Async Interaction Cycle

A complete voice interaction round-trip:

```
1. User taps mic button
   → UI: mic icon changes to stop icon, SoundWaves animation starts

2. User speaks, taps stop
   → Recording stops, isProcessing = true
   → UI: processing indicator shown

3. WAV uploaded for transcription
   → Backend returns transcript
   → UI: transcript populates query input, isProcessing = false

4. User reviews transcript, taps send
   → Text query sent with conversation history + file context
   → UI: typing indicator (BouncingDots) shown

5. API responds with { response, audio_content }
   → Bot message inserted, typing indicator removed
   → Audio playback begins automatically
   → UI: avatar switches to speaking GIF, isSpeaking = true

6. Audio finishes
   → Cache file deleted
   → UI: avatar returns to idle, isSpeaking = false
```

---

## UI Synchronization

### State Variables for Audio UI

| State | Type | Controls |
|---|---|---|
| `isRecording` | boolean | Mic button appearance, SoundWaves visibility |
| `isProcessing` | boolean | Processing spinner after recording stop |
| `isSpeaking` | boolean | Avatar animation (idle vs. speaking GIF) |
| `sound` | Audio.Sound \| null | Active playback instance reference |
| `recording` | Audio.Recording \| null | Active recording instance reference |

### Animation Components

| Component | Purpose | Animation |
|---|---|---|
| `SoundWaves` | Recording visualization | 3 bars with looped height animation (staggered 150ms) using `Animated.timing` |
| `BouncingDots` | AI thinking indicator | 3 dots with looped scale + translateY using `Animated.timing` |

### Avatar State

```
Recording:  → Idle avatar (static image)
Processing: → Idle avatar + ActivityIndicator
Speaking:   → Speaking avatar (animated GIF: ai_talking.gif)
Idle:       → Idle avatar (Cloudinary static image)
```

---

## Bilingual Support

The audio pipeline supports **English** and **Hindi** interaction:

| Language Code | Behavior |
|---|---|
| `en-US` | Standard English query and response |
| `hi-IN` | Query text is appended with a Hindi persona instruction before sending; TTS response is generated in Hindi by the backend |

Language selection is managed via a `Picker` dropdown in the tutor chat UI and passed to the backend in every request.
